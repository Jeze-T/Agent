# 什么是大模型 Agent ？
大模型 Agent，作为一种人工智能体，是具备环境感知能力、自主理解、决策制定及执行行动能力的智能实体。
智能体是一种通用问题解决器，从软件工程的角度看来，智能体是一种基于大语言模型的，具备规划思考能力、记忆能力、自主调用函数的能力，能自主完成给定任务的计算机程序。

# Agent 的思维模式
我们希望智能体也拥有类似人类规划能力的思维模式，因此可以通过 LLM 提示工程，为智能体赋予这样的思维模式。在智能体中，最重要的是让 LLM 具备着以下两个能力：
1、拆解子目标和任务分解方法
2、反思和完善。

## 拆解子目标和任务分解方法
通过大模型提示工程，比如：ReAct、CoT 推理模式，可赋予智能体类似思维模式，精准拆解复杂任务，分步解决。

COT
原论文：https://arxiv.org/pdf/2201.11903.pdf[1]
关于使用COT：现在应该是大模型本身就应该具备的能力，只需要在Prompt 中添加了一句“Let's Step by Step”，将艰难的任务分解成更小，更简单的步骤。
CoT将大型任务转化为多个可管理的任务，并对模型的思维过程进行了阐释。

Self-consistency with CoT（CoT的自我一致性）
原论文：https://arxiv.org/pdf/2203.11171.pdf[2]
一种CoT在实际应用中的方案。简单地要求模型对同一提示进行多次回答，并将多数结果作为最终答案。 它是CoT（Chain of Thought）的后续方法。

TOT
原论文：https://arxiv.org/pdf/2305.10601.pdf[3]
具体结构图如下：虚线左边为基本Prompt、CoT以及CoT Self Consisitency，虚线右边为ToT。
ToT思维树的基本过程：
从原始输入开始，在思维链的每一步，采样多个分支。逐渐形成树状结构，对每一个分支进行评估，然后在合适的分支上进行搜索，探索多种推理的可能性。
它首先将问题分解为多个思考步骤，并在每个步骤中生成多个思考，从而创造一个树形结构。
搜索过程可以是BFS(广度优先搜索)或DFS(深度优先搜索)，每个状态由**分类器(通过一个prompt)**或少数服从多数的投票原则来决定。
任务分解可通过以下几种方式实现：
1、给LLM一个简单的提示词“Steps for XYZ.”，“What are the subgoals for achieving XYZ?”;
2、使用针对具体任务的指令，例如对一个写小说的任务先给出“Write a story outline.”指令;
3、使用者直接输入;

GOT
原论文：https://arxiv.org/pdf/2308.09687[4]
Graph of Thoughts：同时支持多链、树形以及任意图形结构的Prompt方案，支持各种基于图形 的思考转换，如聚合、回溯、循环等，这在CoT和ToT中是不可表达的。
将复杂问题建模为操作图 (Graph of Operations，GoO)，以LLM作为引擎自动执行，从而提供解决复杂问题的能力。某种程度上，GoT囊括了单线条的CoT和多分枝的ToT。
无论是CoT还是ToT，本质上是通过**Prompt的精心设计**，激发出模型原有的Metacognition，只是如何通过某条神经元的线索能更加精准的调动出大脑中最擅长Planning的部分

LLM+P
原论文：https://arxiv.org/pdf/2304.11477[5]
LLM+P：通过借助一个外部的经典Planner来进行一个更加长序列的整体规划。这种方法利用规划域定义语言(Planning Domain Definition Language, PDDL)作为中间接口来描述规划问题。
这是第一个将经典规划器的优点纳入 LLM 的框架。 LLM+P 接收一个规划问题的自然语言描述，然后用自然语言返回一个解决该问题的正确（或最优）计划。
过程如下：
1、首先将语言描述转换为用规划域定义语言（Planning Domain Definition Language）编写的文件。
2、然后利用经典规划器快速找到一个解决方案，再将找到的解决方案翻译成自然语言。
根本上讲， Planning Step是外包给外部工具的，当然也有一个前提：需要有特定领域的PDDL和合适的 Planner。
作者还定义了一组不同的基准问题，这些问题取自常见的规划场景。通过对这些基准问题的全面实验，作者发现LLM+P能够为大多数问题提供最优的解决方案，而 LLM 甚至不能为大多数问题提供可行的计划。

详细介绍如下：
“1. LLM的强项： 大语言模型（如GPT、BERT等）在处理自然语言理解、生成以及处理非结构化问题上表现突出。它们可以推理、生成文本，并根据已有的上下文生成合理的方案，
    特别适合处理灵活且复杂的任务。但是，LLM在面对长序列规划问题时（需要多步骤的复杂推理和任务执行）存在一定的局限性。
    由于LLM是概率模型，随着推理步骤的增加，预测错误的累积会导致模型在长序列任务中的性能下降。
2. Planner的强项： 经典规划器（如A*算法、基于图的规划器等）在处理结构化问题和长序列任务上具有优势。它们可以精确地规划和推理，确保在给定条件下找到最优的执行步骤，
   但它们往往需要明确的规则和约束来进行规划，对非结构化问题的处理能力较弱。
3. LLM+P 的协作：LLM作为任务的高层次决策器，用于理解自然语言描述的复杂任务，识别任务的核心要素，并生成一个初步的任务框架或序列。
   这一过程涉及到从任务描述中推断出目标、子目标和可能的策略。Planner则接管具体的规划细节部分，借助经典规划算法，完成复杂任务的长序列规划。
    在这个阶段，Planner会优化由LLM生成的高层次策略，确保每一步的合理性，并对长序列任务进行整体优化。
经典的Planner算法种类繁多，每种算法都有其特定的应用场景和优势：
A*、Dijkstra和Bellman-Ford等算法适用于图搜索和路径规划问题。
RRT和PRM在处理复杂、高维空间中的机器人运动规划方面表现出色。
HSP、STRIPS、SAT规划器以及PDDL等用于处理更加复杂、基于任务执行的自动规划问题。


## 反思和完善
上述规划模块不涉及任何反馈，这使得实现解决复杂任务的长期规划变得具有挑战性。为了解决这一挑战，可以利用一种机制，使模型能够根据过去的行动和观察反复思考和细化执行计划。
其目标是纠正并改进过去的错误，这有助于提高最终结果的质量。同时反思任务是否已经完成，并终止任务。这在复杂的现实世界环境和任务中尤其重要，其中试错是完成任务的关键。
这种反思或批评机制的两种流行方法包括ReAct和Reflexion。Chain of Hindsight 等。
ReAct通过追踪LLM的推理过程来让模型优化、跟踪和更新行动计划，并能处理异常情况。
Reflextion则比ReAct更进一步，在ReAct的流程中加入推理评估来尝试改进推理结果。
Chain of Hindsight则是通过大量的反馈结果中学习并优化推理结果。

### ReAct(融合推理与执行的能力)
个人理解一下：CoT、ToT 都是作用在大模型本身的内在推理（Reason）过程上，而 ReAct 则是统筹整个系统，从推理过程，结合外部工具共同实现最终的目标（Reason + Action），
通过结合推理（Reasoning）和行动（Acting）来增强推理和决策的效果。
推理（Reasoning）： LLM 基于「已有的知识」或「行动（Acting）后获取的知识」，推导出结论的过程。
行动（Acting）： LLM 根据实际情况，使用工具获取知识，或完成子任务得到阶段性的信息。
原论文：https://arxiv.org/pdf/2210.03629[6]
以ReAct论文中下面这张图来看，可以更清晰的理解ReAct与CoT、ToT的区别：
对于ReAct这个框架可以理解为是一种结合了推理和行动的新型人工智能框架，主要用于增强AI系统在复杂环境中的决策能力和执行效率。
ReAct框架的核心思想是通过实时检索相关信息和执行基于这些信息的行动，来辅助AI系统进行更准确的推理和决策。

为什么结合推理和行动，就会有效增强 LLM 完成任务的能力？
仅推理（Reasoning Only）：LLM 仅仅基于已有的知识进行推理，生成答案回答这个问题。很显然，如果 LLM 本身不具备这些知识，可能会出现幻觉，胡乱回答一通。
仅行动（Acting Only）： 大模型不加以推理，仅使用工具（比如搜索引擎）搜索这个问题，得出来的将会是海量的资料，不能直接回到这个问题。
推理+行动（Reasoning and Acting）： LLM 首先会基于已有的知识，并审视拥有的工具。当发现已有的知识不足以回答这个问题，
                                  则会调用工具，比如：搜索工具、生成报告等，然后得到新的信息，基于新的信息重复进行推理和行动，直到完成这个任务。

在ReAct框架中，AI系统不仅依赖于其预训练的知识，还会在遇到新情况时，主动检索外部信息（如数据库、网络资源等），并将这些信息整合到其决策过程中。
这一过程可以看作是AI系统在“思考”（Reasoning）和“行动”（Acting）之间的循环，其中：
思考（Reasoning）：AI系统基于当前状态和目标，进行推理和规划，确定下一步需要采取的行动或需要检索的信息。
行动（Acting）：根据推理结果，AI系统执行相应的行动，如检索信息、执行任务等。
反馈：AI系统根据行动的结果，更新其状态和知识，然后再次进入思考阶段，形成一个闭环。
ReAct框架的优势：它使AI系统能够适应不断变化的环境，处理之前未见过的情况，而不仅仅是依赖于预训练数据。
                通过实时检索和整合新信息，AI系统可以做出更准确、更灵活的决策，提高其在复杂任务中的表现。


### Reflexion(动态记忆和自我反思)
原论文：https://arxiv.org/pdf/2303.11366[7]
Reflexion是一个框架，旨在通过赋予智能体**动态记忆和自我反思能力**来提升其推理技巧。
该方法采用标准的**强化学习（RL）设置**，其中奖励模型提供简单的**二元奖励**，行动空间遵循ReAct中的设置，即通过语言增强特定任务的行动空间，以实现复杂的推理步骤。
每执行一次行动后，智能体会计算一个启发式评估，并根据自我反思的结果，可选择性地重置环境，以开始新的尝试。启发式函数用于确定轨迹何时效率低下或包含幻觉应当停止。
效率低下的规划指的是长时间未成功完成的轨迹。幻觉定义为遭遇一系列连续相同的行动，这些行动导致在环境中观察到相同的结果。



